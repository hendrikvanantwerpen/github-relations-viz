\documentclass[10pt,a4paper]{article}

\usepackage[english]{babel}
\renewcommand*\ttdefault{txtt}
\usepackage[T1]{fontenc}
\usepackage[round,authoryear]{natbib}
\usepackage[hidelinks]{hyperref}

\usepackage{listings}
\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,do,else,extends,false,
                final,finally,for,if,implicit,import,match,mixin,new,
                null,object,override,package,private,protected,
                requires,return,sealed,super,this,trait,true,try,
                type,val,var,while,with,yield},
  otherkeywords={=,=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,  
  morecomment=[l]//,
  morecomment=[n]{/*}{*/}, %or [s]
  morestring=[b]",
  morestring=[b]',
  morestring=[b]"""
}
\lstset{
  %frame=tb,
  language=scala,
  %aboveskip=3mm,
  %belowskip=3mm,
  showstringspaces=false,
  numbers=left,
  breaklines=false,
  columns=flexible,
  captionpos=t,
  basicstyle={\small\ttfamily}
}

\title{Visualizing Project Relations on GitHub}
\author{
    Hendrik van Antwerpen\thanks{\href{mailto:H.vanAntwerpen@student.tudelft.nl}{\nolinkurl{H.vanAntwerpen@student.tudelft.nl}}}
  \and
    Niels van Kaam\thanks{\href{mailto:N.vanKaam@student.tudelft.nl}{\nolinkurl{N.vanKaam@student.tudelft.nl}}}
}
\date{Software Engineering Research Group, EECMS\\Delft University of Technology}

\begin{document}

\maketitle

\begin{abstract}
GitHub has emerged as an interesting object of study for software repository analysis because of its open API. Visualizing this data can be challenging, both because of the lack of knowledge about its structure and because of the size of the dataset. In this report we describe a web based system that makes it possible to explore one such relation, the links between projects based on common committers. Users can explore the data by selecting time limits, link degrees and project language. The project was developed in the context of a course in functional programming with extra focus on distributed data processing and the report describes the functional and distributed techniques used to build the data processing.
\end{abstract}

\section{Introduction}

In the field of software repository mining, GitHub has emerged as an interesting subject of study and tools have emerged to collect and publish the data it publishes \citep{gousios2012ghtorrent}. Making sense of the data and visualizing it in interesting ways is not trivial. An interactive interface to explore relations in the data set, allowing a researcher to play with parameters like project language or time period, can be a good entry point to further investigation of the data using formal statistic methods.

In this report we present a web based software system that allows a user to explore relations between projects on GitHub. Projects have a relation when a common committer exists. The user can influence the time period where the link must exists, the main programming language of the project and the minimum degree that is included in the results.

We identify several challenges in the design of the software:
\begin{itemize}
    \item Providing a general visualization that might reveal interesting properties of the data to the user.
    \item Dealing with the large size of the data that is being processed.
    \item Ensuring responsiveness of the software, so exploring the data is easy and practical.
\end{itemize}

The software was developed in the context of a functional programming course\footnote{\url{http://swerl.tudelft.nl/bin/view/Main/FunctionalProgrammingCourse}} at the Delft University of Technology. The course focused on functional programming techniques, application of these techniques in more imperative languages and using them for processing large data sets. The report therefore also describes how the functional programming and data processing techniques from the course were applied. The project was implemented using Scala and the code samples will be mostly in that language.

We describe the following aspects of the software:
\begin{itemize}
    \item A description of the data and the data processing (section \ref{sec:data}).
    \item A description of a MapReduce implementation in Scala, created to implement the data processing (section \ref{sec:mapreduce}).
    \item A description of the distributed design of the backend, created to improve responsiveness of the software (section \ref{sec:distributed}).
    \item A description of the visualization used in the software (section \ref{sec:visualization}).
    \item Conclusions and ideas for further research and development (section \ref{sec:conclusions}).
\end{itemize}

\section{GitHub data analysis}\label{sec:data}

The data we could use was a list of all projects with their main programming language, a list of all users and a list of commits. Every commit links a user to a project at some time.

% some data statistics?

We allow the user to vary the following parameters:
\begin{itemize}
    \item Project language (limits projects within one ecosystem)
    \item Time period in which the links are counted
    \item Minimum link degree
\end{itemize}
The desired end result is a list of links between projects, a weight on each link of the amount of common committers and a weight on each project of the amount of connected projects. This last property can identify central projects in the graph.

Processing data can be elegantly expressed as a stream of filter and transform operations. For our purpose they are:
\begin{enumerate}
    \item filter projects for selected language
    \item filter commits and exclude commits outside of the selected time period or not in the filtered project list
    \item create a set of projects per user
    \item per user, create links between all those projects
    \item merge all project links, assigning them weight based on their occurrence
    \item transform the list of links to a format understood by the visualization code
\end{enumerate}

\section{MapReduce with Scala collections}\label{sec:mapreduce}

An increasingly popular approach for processing big data sets is using a form of MapReduce. This lends itself very well for distribution of the data processing. One incarnation of MapReduce is found in Google's Sawzall DSL, using a form of aggregators. A rigorous specification of this model is given in \cite{lammel2008google}. This describes the Sawzall model formally using monoids, that can be hierarchically composed for distribution of the workload. Because monoids for common data types like lists, maps and tuples are readily available, developers don't need to focus much on the reduce phase and can focus on the map phase. Because Sawzall also allows map functions to output one element of a collection to be appended, L\"ammel defines aggregators in terms of generalized monoids:
\begin{lstlisting}[language=haskell]
class Monoid m => Aggregator e m | e -> m
  where
    minsert :: e -> m -> m
\end{lstlisting}

\subsection{Aggregators}

Since to our knowledge no implementation of this approach was available in Scala, we set out to implement it ourselves. Although L\"ammel defines aggregators as a subtype of monoids, we use the aggregator as the only type. A monoid becomes an aggregator where e and m are of the same type. Why this is useful will become clear later. For now, the base trait is:
\begin{lstlisting}
trait Aggregator[A,B] {
  def zero: A
  def insert(a: A, b: B): A
}
\end{lstlisting}

Using Scala implicits we have defined \lstinline!a |<| b! as the Aggregator insert operator. Whenever this operator is used, a fitting Aggregator is inferred. All Scala \lstinline|implicit| rules apply, so if no Aggregator is found there's compile error, if multiple are found the programmer has to specify which one to use.

Assuming we have an appending aggregator for \lstinline|List| and a summing one for \lstinline|Int|, we can do things like the following:
\begin{lstlisting}
val r:Int = i:Int |<| j:Int // sum ints
val r:List[Int] = l:List[Int] |<| m:List[Int] // concat lists
val r:List[Int] = l:List[Int] |<| i:Int // append int element to list
\end{lstlisting}

Because of the emphasize on generics in the Scala collections library (see \citep{odersky2009fighting} for more details), we were able to provide aggregators for all Scala collections (including parallel ones) by implementing only seven aggregators. For \lstinline|Set|, \lstinline|Seq| and \lstinline|Map| each one aggregator for monoid behaviour and one for element insertion. Because of the design of the collections library, every collection is a Traversable of it's element type, so the monoidal aggregators can be liberal in their input, which allows things like:
\begin{lstlisting}
val r:List[Int] = l:List[Int] |<| s:Set[Int]
val r:TreeMap[Int,Int] = tm:TreeMap[Int,Int] |<| sm:SortedMap[Int,Int]
//val r:Map[Int,Int] = m:Map[Int,Int] |<| l:List[(Int,Int)] // doesn't work?
\end{lstlisting}
The last example is something that is expected to work (\lstinline|Map[K,V] <: Traversable[(K,V)]|, but is somehow not inferred by the compiler. Discussions online suggest that the compiler doesn't consider the complete type hierarchy when inferring, but only a few levels. To also allow this construction the seventh aggregator was implemented.

A nice property of monoids is that they can be combined. E.g. a tuple with two monoid values is itself a monoid. Let's see how this property manifests itself in our aggregator world. Aggregators for \lstinline|TupleN| and \lstinline|Map| where implemented to require their tuple values and map value to be aggregators themselves. Here is where the choice to abandon a seperate monoid type for a aggregator pays of. When creating deep structures of values, on every level we have the choice to insert either a value or a collection. Some examples should make this more clear:
\begin{lstlisting}
// inserted tuples contain collections or elements. values are independent
val r:(Set[Int],Set[String]) = t1:(Set[Int],Set[String]) |<| t2:(Set[Int],Set[String])
val r:(Set[Int],Set[String]) = t1:(Set[Int],Set[String]) |<| t2:(Int,String)
val r:(Set[Int],Set[String]) = t1:(Set[Int],Set[String]) |<| t2:(Set[Int],String)

// three level deep structures
val m:Map[Int,Set[String]] = Map[Int,Set[String]] |<| Map[Int,Set[String]]
val m:Map[Int,Set[String]] = Map[Int,Set[String]] |<| Map[Int,String]
val m:Map[Int,Set[String]] = Map[Int,Set[String]] |<| (Int,Set[String])
val m:Map[Int,Set[String]] = Map[Int,Set[String]] |<| (Int,String)
\end{lstlisting}

Our design allows a lot of freedom in the shape of the elements inserted into a collection. Which constructs are allowed is dictated by the defined aggregators and fully inferred by the compiler. The options for the Scala collections are very generic and the developer doesn't have to care about merging the collections. For opaque values aggregators are implemented for string concatenation and integer summing, but others are very easy to implement; one implicit function and an implementation of the two Aggregator functions is enough.

\subsection{MapReduce}

Using the aggregators we now have, a map-reduce library was implemented. Ideally the map-reduce functionality was as easy to use as the standard collection functions, like \lstinline|map|, \lstinline|filter| and \lstinline|partition|. To a high degree this can be achieved by Pimp-my-library pattern, described in \citep{odersky2006pimp}. Again the design of the collections library plus type inference allow us to write map-reduce by only specifying the expected result type. This and the return type of the map function is enough to find the necessary aggregator. A word count example similar to one L\"ammel gives is:
\begin{lstlisting}
val docs: List[String]
def wordcount(doc: String): List[(String,Int)] = doc.split(" ").toList.map( w => (w,1) )
val wc = docs.mapReduce[Map[String,Int]]( wordcount )
// wc has type Map[String,Int] here
\end{lstlisting}

The performance of our map-reduce in the same order and range as hand written code using e.g. \lstinline|foldLeft|. Most of the work is done through inference by the compiler. At runtime the aggregators use folds and collection operations just as you would in handwritten code. There a little overhead of some function calls, but all the reduction details are abstracted away, resulting in less repetition and simpler data transformation functions.

With this result we can write our processing steps from section \ref{sec:data} in Scala. You can see they mix seamless with the existing collection library functions.
\begin{lstlisting}
val commits: List[Commit] = ...
def projectsToPairs(ps: List[Project]): List[(Project,Project)] = ...
val linkMap = 
  commits.filter( c => c.timestamp >= from && c.timestamp <= until  )
         .mapReduce[Map[User,Set[Project]]]( c => (c.user,c.project) )
         .values
         .mapReduce[Map[(Project,Project),Int]](projectsToParis)
         .filter( _._2 >= minDegree )
\end{lstlisting}

\section{Distributed data-processing with Akka}\label{sec:distributed}

\begin{itemize}
    \item What did we want to address
    \item How did we design the actors, where was the caching, where which processing steps
    \item What was the performance (did we speed up response time?)
    \item A design diagram
\end{itemize}

\section{Visualization in the browser with D3}\label{sec:visualization}

\begin{itemize}
    \item What did we want to show?
    \item How did we implement it (D3, what graph parameters)
    \item How did we emphasize characteristics (e.g. central projects)
    \item An example result
\end{itemize}

\section{Conclusions and further research}\label{sec:conclusions}

\begin{itemize}
    \item Did it work
    \item Is it responsive?
    \item Did we show interesting things
    \item What more interesting things could fit in this framework?
\end{itemize}

We believe the given approach can be easily adapted to include other parameters or show other relations in the data.

\bibliographystyle{abbrvnat}
\bibliography{github-relations-viz}

\end{document}